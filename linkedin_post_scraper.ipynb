{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89eb7a18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Scraping-linkedin-Posts\" data-toc-modified-id=\"Scraping-linkedin-Posts-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Scraping linkedin Posts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Utils\" data-toc-modified-id=\"Utils-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Utils</a></span></li><li><span><a href=\"#Login\" data-toc-modified-id=\"Login-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Login</a></span></li><li><span><a href=\"#Load-posts-page-&amp;-scroll-to-bottom\" data-toc-modified-id=\"Load-posts-page-&amp;-scroll-to-bottom-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Load posts page &amp; scroll to bottom</a></span></li><li><span><a href=\"#Retrieve-data-from-loaded-page\" data-toc-modified-id=\"Retrieve-data-from-loaded-page-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Retrieve data from loaded page</a></span></li><li><span><a href=\"#Saving-blog-posts-to-files\" data-toc-modified-id=\"Saving-blog-posts-to-files-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Saving blog posts to files</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137062f",
   "metadata": {},
   "source": [
    "# Scraping linkedin Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5063146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from selenium import webdriver\n",
    "except:\n",
    "    %pip install selenium\n",
    "    from selenium import webdriver\n",
    "\n",
    "try:\n",
    "    import unidecode\n",
    "except:\n",
    "    %pip install unidecode\n",
    "    import unidecode\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    %pip install pandas\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc478841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re as re\n",
    "from importlib.metadata import version\n",
    "from typing import Any, Dict, Optional\n",
    "import os\n",
    "import types\n",
    "import logging\n",
    "import tkinter as tk\n",
    "import json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ba83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE = 'https://www.linkedin.com/company/mgm-technology-partners-gmbh'\n",
    "SCROLL_PAUSE_TIME = 1.5\n",
    "DATA_DIRECTORY = os.getenv('DATA_DIRECTORY') or 'data'\n",
    "os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
    "\n",
    "BLOGS_DIRECTORY = os.getenv('BLOGS_DIRECTORY') or f\"{DATA_DIRECTORY}/blogs\"\n",
    "os.makedirs(BLOGS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "TMP_DIRECTORY = os.getenv('TMP_DIRECTORY') or f\"{DATA_DIRECTORY}/tmp_linkedin\"\n",
    "os.makedirs(TMP_DIRECTORY, exist_ok=True)\n",
    "\n",
    "FILENAME_SOUP = \"linkedin_soup.html\"\n",
    "INTERNAL_DATE_FORMAT = \"%Y-%m-%d\"\n",
    "NO_DATE = \"__no_date__\"\n",
    "\n",
    "FILENAME_RAW_POSTS = f\"{TMP_DIRECTORY}/raw_posts.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f564b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f= open(\"credentials.txt\",\"r\")\n",
    "    contents = f.read()\n",
    "    username = contents.replace(\"=\",\",\").split(\",\")[1]\n",
    "    password = contents.replace(\"=\",\",\").split(\",\")[3]\n",
    "except:\n",
    "    f= open(\"credentials.txt\",\"w+\")\n",
    "    username = input('Enter your linkedin username: ')\n",
    "    password = input('Enter your linkedin password: ')\n",
    "    f.write(\"username={}, password={}\".format(username,password))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298c7d6",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e63214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name, log_level=logging.WARN):\n",
    "    # Get a logger with the given name\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.propagate = False  # Disable propagation to the root logger. Makes sense in Jupyter only...\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # Check if the logger has handlers already\n",
    "    if not logger.handlers:\n",
    "        # Create a handler\n",
    "        handler = logging.StreamHandler()\n",
    "\n",
    "        # Set a format that includes the logger's name\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "    \n",
    "def transformDate2String(dateToTransform: datetime) -> str:\n",
    "    logger = get_logger(transformDate2String.__name__)\n",
    "    try:\n",
    "        dateStr = dateToTransform.strftime(INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        logger.error(f\"Error transforming date: {dateToTransform}. Continuing with empty date string.\")\n",
    "        dateStr = \"\"\n",
    "    return dateStr\n",
    "\n",
    "def transformString2Date(stringToTransform: str) -> Optional[datetime]:\n",
    "    \"\"\"Transforms a String that holds a date in my standard format to a Date. \n",
    "        In case it can't transform it, it return None.\"\"\"\n",
    "    try:\n",
    "        dateObj = datetime.strptime(stringToTransform, INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        log(\"transformString2Date\", \"Error transforming string to date: \",\n",
    "            stringToTransform)\n",
    "        dateObj = None\n",
    "    return dateObj\n",
    "\n",
    "def getNowAsString() -> str:\n",
    "    return transformDate2String(datetime.now())\n",
    "\n",
    "def getMinDateAsString() -> str:\n",
    "    return transformDate2String(datetime(1970, 1, 1))\n",
    "\n",
    "def stripBlanks(str):\n",
    "    return str.strip(\" \\t\")\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392fee1f-7082-4f06-9a4c-2af8c18abb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 07:55:16,161 - readDictFromFile - WARNING - Could not open file test.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, I am in an exception as I expected it to be\n"
     ]
    }
   ],
   "source": [
    "def writeDictToFile(*, dictionary: Dict, fullFilename: str) -> Dict:\n",
    "    \"\"\"Writes a dictionary to a file. Also updates the _stats element.\"\"\"\n",
    "    logger = get_logger(writeDictToFile.__name__, logging.INFO)\n",
    "    if not isinstance(dictionary, dict):\n",
    "        raise TypeError(\"Expected a dictionary, but got a \" + str(type(dictionary)))\n",
    "    #log(\"writeDictToFile\", \"Len of dict to write: \", len(dictionary), \" type: \", type(dictionary))\n",
    "    nowStr = getNowAsString()\n",
    "    dictionary.setdefault(\"_stats\", {\"lastWritten\": nowStr})\n",
    "    dictionary[\"_stats\"][\"lastWritten\"] = nowStr\n",
    "    dictionary[\"_stats\"][\"counter\"] = len(dictionary)-1\n",
    "    stats = dictionary[\"_stats\"]\n",
    "    del dictionary[\"_stats\"]\n",
    "    #log(\"writeDictToFile\", \"Len of dict after deleting _stats: \", len(dictionary), \" type: \", type(dictionary))\n",
    "    dictionary = dict(sorted(dictionary.items()))\n",
    "    #log(\"writeDictToFile\", \"Len of dict after sorting: \", len(dictionary), \" type: \", type(dictionary))\n",
    "    sortedDictionary = {\"_stats\": stats, **dictionary}\n",
    "    #log(\"writeDictToFile\", \"Len of sorted dict to write: \", len(sortedDictionary), \" type: \", type(dictionary))\n",
    "    dictDump = json.dumps(sortedDictionary, sort_keys=False, indent=2)\n",
    "\n",
    "    # Make sure that the directory in which we want to write exists.\n",
    "    directory = os.path.dirname(os.path.abspath(fullFilename))\n",
    "    #log('writeDictToFile', 'Writing to dir ', directory)\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except FileExistsError:\n",
    "        # directory already exists, so no need to create it - all good\n",
    "        pass\n",
    "\n",
    "    with open(fullFilename, 'w') as file:\n",
    "        file.write(dictDump)\n",
    "    return sortedDictionary\n",
    "\n",
    "def readDictFromFile(*, fullFilename: str) -> Dict:\n",
    "    \"\"\"Reads a dictionary from a file. Chacks that the dictionary read has a _stats.lastWritten entry.\"\"\"\n",
    "    logger = get_logger(readDictFromFile.__name__, logging.INFO)\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(fullFilename, \"r+\") as file:\n",
    "            data = json.load(file)\n",
    "            if data == None: \n",
    "                return {}\n",
    "            if data.get(\"_stats\", {}).get(\"lastWritten\") == None:\n",
    "                logger.warning(f\"Read file {fullFilename} successfully but does not contain _stats.lastWritten.\")\n",
    "            return data\n",
    "    except IOError as e:\n",
    "        logger.warning(f\"Could not open file {fullFilename}\")\n",
    "        raise e\n",
    "    return data\n",
    "\n",
    "def test_writeDictToFile():\n",
    "    data = {\n",
    "        \"hello\": \"world\",\n",
    "        \"now\": \"what\"\n",
    "    }\n",
    "    writeDictToFile(dictionary=data, fullFilename=\"test.json\")\n",
    "\n",
    "def test_readDictFromFile():\n",
    "    data = {\n",
    "        \"hello\": \"world\",\n",
    "        \"now\": \"what\"\n",
    "    }\n",
    "    FILENAME = \"test.json\"\n",
    "\n",
    "    # Write and then read it\n",
    "    writeDictToFile(dictionary=data, fullFilename=FILENAME)\n",
    "    data2 = readDictFromFile(fullFilename=FILENAME)\n",
    "\n",
    "    # Delete test data, try to read it - even though it doesn't exist\n",
    "    try:\n",
    "        os.remove(FILENAME)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        data3 = readDictFromFile(fullFilename=FILENAME)\n",
    "    except:\n",
    "        print(\"All good, I am in an exception as I expected it to be\")\n",
    "\n",
    "test_readDictFromFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20e1d0",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2850cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loggedin_browser():\n",
    "    #access Webriver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    #chrome_options.add_argument(\"--headless=new\")\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    #Open login page\n",
    "    browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "    \n",
    "    #Enter login info:\n",
    "    elementID = browser.find_element(by=By.ID, value='username')   #.find_element_by_id('username')\n",
    "    elementID.send_keys(username)\n",
    "    \n",
    "    elementID = browser.find_element(by=By.ID, value='password')#find_element_by_id('password')\n",
    "    elementID.send_keys(password)\n",
    "    #Note: replace the keys \"username\" and \"password\" with your LinkedIn login info\n",
    "    elementID.submit()\n",
    "    return browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc679d3",
   "metadata": {},
   "source": [
    "## Load posts page & scroll to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea79bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def browser_go_to_page(browser, max_pages=0):\n",
    "    logger = get_logger(browser_go_to_page.__name__, logging.INFO)\n",
    "    #Go to webpage\n",
    "    company_posts_page = PAGE + '/posts/'\n",
    "    logger.info(f\"{company_posts_page=}\")\n",
    "    browser.get(company_posts_page)\n",
    "    \n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_page = 0\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        #click_visible_menues(browser)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        scroll_page += 1\n",
    "        logger.info(f\"Scrolling page {scroll_page}\")\n",
    "        \n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "    \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        if max_pages > 0:\n",
    "            if scroll_page == max_pages:\n",
    "                break\n",
    "                \n",
    "    return \n",
    "\n",
    "def get_page_source(browser, max_pages=0):\n",
    "    logger = get_logger(get_page_source.__name__, logging.INFO)\n",
    "    browser_go_to_page(browser, max_pages)\n",
    "\n",
    "    company_page = browser.page_source   \n",
    "    return company_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0365b2ff-716a-4545-a49b-256fcdf35efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkedin_browser(max_pages=0):\n",
    "    browser = get_loggedin_browser()\n",
    "    browser_go_to_page(browser, max_pages=max_pages)\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72975e00",
   "metadata": {},
   "source": [
    "## Retrieve data from loaded page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ce246d-efb0-4c4c-931c-37ecaff6377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_container_elements(max_pages):\n",
    "    logger = get_logger(retrieve_container_elements.__name__, logging.INFO)\n",
    "    browser = get_linkedin_browser(max_pages=max_pages)\n",
    "    container_elements = browser.find_elements(By.CLASS_NAME, \"occludable-update\")\n",
    "    logger.info(f\"No of container elements before filter: {len(container_elements)}\")\n",
    "    container_elements = [element for element in container_elements if len(element.find_elements(By.CLASS_NAME,\"update-components-actor\")) > 0]\n",
    "    logger.info(f\"No of container elements after filter: {len(container_elements)}\")\n",
    "    return container_elements, browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518b336b-02d5-4d13-9f02-345cde1b035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_element_in_viewport(driver, element):\n",
    "    return driver.execute_script(\"\"\"\n",
    "        var elem = arguments[0];\n",
    "        var rect = elem.getBoundingClientRect();\n",
    "        return (\n",
    "            rect.top >= 0 &&\n",
    "            rect.left >= 0 &&\n",
    "            rect.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&\n",
    "            rect.right <= (window.innerWidth || document.documentElement.clientWidth)\n",
    "        );\n",
    "    \"\"\", element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eeb008c-dc2a-426d-99ae-fa3bf1af7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_url(browser):\n",
    "    logger = get_logger(get_post_url.__name__, logging.WARN)\n",
    "    elements = browser.find_elements(By.XPATH, \"//*[text()='Copy link to post']\")\n",
    "    if len(elements) != 1:\n",
    "        logger.warning(f\"Number of list of elements that should give me the URL of the blogpost: {len(elements)}\")\n",
    "        return None\n",
    "    try:\n",
    "        elements[0].click()\n",
    "        root = tk.Tk()\n",
    "        blog_post_url = root.clipboard_get()\n",
    "        logger.info(f\"URL of blog post: {blog_post_url}\")\n",
    "        return blog_post_url\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"Could not extract blog post url, retrurning None. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_blog_post_url_from_container_element(browser, container_element):\n",
    "    logger = get_logger(extract_blog_post_url_from_container_element.__name__, logging.INFO)\n",
    "    #logger.info(f\"Extracting from container of type {type(container_element)}\")\n",
    "    buttons = container_element.find_elements(By.CLASS_NAME, 'feed-shared-control-menu__trigger')  \n",
    "    if len(buttons) != 1:\n",
    "        logger.info(f\"No of buttons found in container: {len(buttons)}. Cannot process this container.\")\n",
    "        return None\n",
    "        \n",
    "    button = buttons[0]\n",
    "    actions = ActionChains(browser)\n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    browser.execute_script('arguments[0].scrollIntoView({ behavior: \"smooth\", block: \"center\", inline: \"nearest\" });', button)\n",
    "    \n",
    "    if not button.is_displayed():  \n",
    "        logger.warn(\"Button not displayed, cannot process container\")\n",
    "        return None\n",
    "        \n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    time.sleep(1)  \n",
    "    button.click()\n",
    "    time.sleep(5)  \n",
    "    url = get_post_url(browser)                \n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83f970c3-778b-4a5e-bcd6-da6510f17f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_blog_containers_to_file(blogs):\n",
    "    logger = get_logger(write_blog_containers_to_file.__name__, logging.INFO)\n",
    "    # Prepare blogs to be saveable i.e. serializable\n",
    "    blogs_to_save = {}\n",
    "    for blog_id, blog in blogs:\n",
    "        blog_to_save = blog\n",
    "        blog_to_save[\"soup\"] = blog[\"soup\"].prettify()\n",
    "        blogs_to_save[blog_id] = blog_to_save\n",
    "    try:\n",
    "        writeDictToFile(dictionary=blogs_to_save,fullFilename=FILENAME_RAW_POSTS)\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"could not write {len(blogs)} blog containers to file {FILENAME_RAW_POSTS}: {e}\")        \n",
    "    return \n",
    "\n",
    "def read_blog_containers_from_file():\n",
    "    logger = get_logger(read_blog_containers_from_file.__name__, logging.INFO)\n",
    "    blogs = {}\n",
    "    try:\n",
    "        blogs = readDictFromFile(fullFilename=FILENAME_RAW_POSTS)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read blog containers from file {FILENAME_RAW_POSTS}. Raising Error.\")\n",
    "        raise e\n",
    "    for blog_id, blog in blogs:\n",
    "        blog[blog_id][\"soup\"] = bs(blog[\"soup\"], \"html.parser\")  # convert string to BeautifulSoup object\n",
    "    logger.info(f\"Read {len(blogs)} blog containersc from file {FILENAME_RAW_POSTS}.\")\n",
    "    return blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b2a89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_soup(soup: bs):\n",
    "    logger = get_logger(extract_text_from_soup.__name__, log_level=logging.INFO)\n",
    "\n",
    "    # In 'container', find the first <div> element with class 'feed-shared-update-v2__description-wrapper'.\n",
    "    # Assign this element to 'text_box'.\n",
    "    text_box = soup.find(\"div\", {\"class\":\"feed-shared-update-v2__description-wrapper\"})\n",
    "    \n",
    "    # If 'text_box' is not None (i.e., if such an element was found in 'container')...\n",
    "    if text_box:\n",
    "        # ...find the first <span> element within 'text_box' that has the 'dir' attribute set to 'ltr'.\n",
    "        # Extract its text content, strip leading and trailing whitespace, and assign this cleaned text to 'text'.\n",
    "        text = text_box.find(\"span\", {\"dir\":\"ltr\"}).text.strip()\n",
    "        \n",
    "        # Return 'text'.\n",
    "        return text\n",
    "    else:\n",
    "        # If 'text_box' is None (i.e., if no such <div> element was found in 'container')...\n",
    "        # ...print an error message.\n",
    "        logger.warning(f\"Could not extract text from soup!\")\n",
    "        \n",
    "        # Uncomment the following line to print the 'container' for debugging purposes.\n",
    "        # print(f\"Container: {container}\")\n",
    "        \n",
    "        # Return an empty string.\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "301e669b-5cff-42ec-af7c-5ceb5edc90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id_from_text(blog_text):\n",
    "    # Create a hash of the blog source\n",
    "    hash_object = hashlib.sha256(blog_text.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return hex_dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981ac4f7-2dcd-4e88-be6a-07ce6b19084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blogs_from_container_elements(browser, container_elements):\n",
    "    logger = get_logger(extract_blogs_from_container_elements.__name__, logging.INFO)\n",
    "    blogs = {}\n",
    "    for container_element in container_elements:\n",
    "        blog_url = extract_blog_post_url_from_container_element(browser, container_element)\n",
    "        blog_source = container_element.get_attribute('outerHTML')\n",
    "        blog_soup = bs(blog_source.encode(\"utf-8\"), \"html\")\n",
    "        blog_text = extract_text_from_soup(blog_soup)\n",
    "        if (len(blog_text) == 0):\n",
    "            logger.warning(f\"Cannot extract text from container, so container has no value and is skipped\")\n",
    "        else:\n",
    "            blog_id = generate_id_from_text(blog_text)\n",
    "            blog = {\n",
    "                \"url\": blog_url,\n",
    "                \"source\": blog_source,\n",
    "                \"soup\": blog_soup,\n",
    "                \"scrape_date\": getNowAsString()\n",
    "            }\n",
    "            blogs[blog_id] = blog\n",
    "                         \n",
    "    logger.info(f\"No of extracted blogs: {len(blogs)}\")\n",
    "    write_blog_containers_to_file(blogs)\n",
    "    return blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "427c0b94-1dd1-4054-b1f6-3356b0e115b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 08:24:25,371 - get_blog_containers - WARNING - Could not read blog containers from file, retrieving from website\n",
      "2023-09-11 08:24:31,731 - browser_go_to_page - INFO - company_posts_page='https://www.linkedin.com/company/mgm-technology-partners-gmbh/posts/'\n",
      "2023-09-11 08:24:33,025 - browser_go_to_page - INFO - Scrolling page 1\n",
      "2023-09-11 08:24:35,221 - browser_go_to_page - INFO - Scrolling page 2\n",
      "2023-09-11 08:24:36,736 - browser_go_to_page - INFO - Scrolling page 3\n",
      "2023-09-11 08:24:38,255 - retrieve_container_elements - INFO - No of container elements before filter: 23\n",
      "2023-09-11 08:24:38,373 - retrieve_container_elements - INFO - No of container elements after filter: 13\n",
      "2023-09-11 08:25:59,064 - extract_blogs_from_container_elements - INFO - No of extracted blogs: 13\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m, in \u001b[0;36mget_blog_containers\u001b[0;34m(force_retrieval, max_pages)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     blog_containers \u001b[38;5;241m=\u001b[39m \u001b[43mread_blog_containers_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blog_containers\n",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m, in \u001b[0;36mread_blog_containers_from_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blog_id, blog \u001b[38;5;129;01min\u001b[39;00m blogs:\n\u001b[1;32m     24\u001b[0m     blog[blog_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bs(blog[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoup\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# convert string to BeautifulSoup object\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         blog_containers \u001b[38;5;241m=\u001b[39m extract_blogs_from_container_elements(browser, container_elements)   \n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m blog_containers\n\u001b[0;32m---> 17\u001b[0m blog_container \u001b[38;5;241m=\u001b[39m \u001b[43mget_blog_containers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_retrieval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m, in \u001b[0;36mget_blog_containers\u001b[0;34m(force_retrieval, max_pages)\u001b[0m\n\u001b[1;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not read blog containers from file, retrieving from website\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m container_elements, browser \u001b[38;5;241m=\u001b[39m retrieve_container_elements(max_pages)\n\u001b[0;32m---> 14\u001b[0m blog_containers \u001b[38;5;241m=\u001b[39m \u001b[43mextract_blogs_from_container_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontainer_elements\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blog_containers\n",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m, in \u001b[0;36mextract_blogs_from_container_elements\u001b[0;34m(browser, container_elements)\u001b[0m\n\u001b[1;32m     19\u001b[0m         blogs[blog_id] \u001b[38;5;241m=\u001b[39m blog\n\u001b[1;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo of extracted blogs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(blogs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mwrite_blog_containers_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blogs\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mwrite_blog_containers_to_file\u001b[0;34m(blogs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Prepare blogs to be saveable i.e. serializable\u001b[39;00m\n\u001b[1;32m      4\u001b[0m blogs_to_save \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blog_id, blog \u001b[38;5;129;01min\u001b[39;00m blogs:\n\u001b[1;32m      6\u001b[0m     blog_to_save \u001b[38;5;241m=\u001b[39m blog\n\u001b[1;32m      7\u001b[0m     blog_to_save[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m blog[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoup\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mprettify()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def get_blog_containers(force_retrieval=False, max_pages=0):\n",
    "    logger = get_logger(get_blog_containers.__name__, logging.INFO)\n",
    "    if force_retrieval:\n",
    "        logger.info(f\"Retrieving blog containers: {force_retrieval=} {max_pages=}\")\n",
    "        container_elements, browser = retrieve_container_elements(max_pages)\n",
    "        blog_containers = extract_blogs_from_container_elements(browser, container_elements)   \n",
    "        return blog_containers\n",
    "    try:\n",
    "        blog_containers = read_blog_containers_from_file()\n",
    "        return blog_containers\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read blog containers from file, retrieving from website\")\n",
    "        container_elements, browser = retrieve_container_elements(max_pages)\n",
    "        blog_containers = extract_blogs_from_container_elements(browser, container_elements)   \n",
    "        return blog_containers\n",
    "\n",
    "blog_container = get_blog_containers(force_retrieval=False, max_pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_string_from_soup(soup: bs):\n",
    "    logger = get_logger(extract_date_string_from_soup.__name__, log_level=logging.WARN)\n",
    "\n",
    "    # Looking for the relative date (in d, w, mo, yr)\n",
    "    # It has the shape: \"1yr •\"\n",
    "    p = re.compile(r'\\d{1,2}(h|d|w|mo|yr)\\s•')\n",
    "    m = re.compile(r'\\d{1,2}(h|d|w|mo|yr)\\s•').search(soup.prettify())\n",
    "    dateHumanReadable = \"\"\n",
    "    if m:\n",
    "        dateHumanReadable = m.group()\n",
    "        logger.info(f\"Match found: {dateHumanReadable}\")\n",
    "        return dateHumanReadable\n",
    "    else:\n",
    "        logger.error(f\"Could not extract human readable date from soup! soup: {soup}\")\n",
    "        return NO_DATE\n",
    "\n",
    "def test_extract_date_string_from_soup():\n",
    "    containers = get_blog_containers()\n",
    "    human_readable_date = extract_date_string_from_soup(containers[0][\"soup\"])\n",
    "    print(human_readable_date)\n",
    "\n",
    "test_extract_date_string_from_soup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ced8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_rel_date2datetime(relative_date):\n",
    "    \"\"\"Transforms a relative date from LinkedIn to a datetime object.\n",
    "    Transform \"6d •\" to a proper datetime\"\"\"\n",
    "\n",
    "    logger = get_logger(linkedin_rel_date2datetime.__name__, log_level=logging.WARN)\n",
    "    \n",
    "    p = re.compile('\\d{1,2}')\n",
    "    m = p.search(relative_date)\n",
    "    if m is None:\n",
    "        logger.error(f\"Amount not found in {relative_date}\")\n",
    "        exit\n",
    "    amount = float(m.group())\n",
    "    p = re.compile('(h|d|w|mo|yr)')\n",
    "    m = p.search(relative_date)\n",
    "    logger.info(f\"m: {m}, type(m): {type(m)}\")\n",
    "    if m is None:\n",
    "        logger.error(f\"Unit not found in {relative_date}\")\n",
    "        exit\n",
    "    unit = m.group()\n",
    "    if unit == 'yr':\n",
    "        amount *= 365*24\n",
    "    elif unit == 'mo':\n",
    "        amount *= 30*24\n",
    "    elif unit == 'w':\n",
    "        amount *= 7*24\n",
    "    elif unit == 'd':\n",
    "        amount *= 24\n",
    "    logger.info(f\" {relative_date} --> Amount in hours: {amount}\")\n",
    "    # Calculate the date from today's, and return it\n",
    "    howRecent = timedelta(hours=amount)\n",
    "    todaysDate = datetime.now()\n",
    "    date = (todaysDate - howRecent)\n",
    "    return date\n",
    "\n",
    "# Some tests\n",
    "rel_dates = ['2h •', '3d •', '1w •']\n",
    "for rel_date in rel_dates:\n",
    "    print(f\"{rel_date} --> {linkedin_rel_date2datetime(rel_date)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_content(content):\n",
    "    content = re.sub('\\n +', '\\n', content)\n",
    "    content = re.sub('\\n+', '\\n\\n', content)\n",
    "    content = content.replace(\"{\", \"&#123;\").replace(\"}\", \"&#125;\")\n",
    "    return content\n",
    "    \n",
    "def extract_all_from_container(container):\n",
    "    logger = get_logger(extract_all_from_container.__name__, logging.INFO)\n",
    "    blog_post = {}\n",
    "    blog_post[\"date_human_readable\"] = extract_date_string_from_soup(container[\"soup\"])\n",
    "    blog_post[\"posted_date\"] = linkedin_rel_date2datetime(blog_post[\"date_human_readable\"])\n",
    "    blog_post[\"text\"] = simplify_content(extract_text_from_soup(container[\"soup\"]))\n",
    "    blog_post[\"original_url\"] = container[\"url\"]\n",
    "    logger.info(f\"{blog_post['posted_date']} - {blog_post['text'][:30]}\")\n",
    "    return blog_post\n",
    "\n",
    "def extract_all_from_containers():\n",
    "    logger = get_logger(extract_all_from_containers.__name__, logging.INFO)\n",
    "    containers = get_blog_containers()\n",
    "    blog_posts = []\n",
    "    \n",
    "    for container_no, container in enumerate(containers):\n",
    "        try:\n",
    "            logger.info(f\"Processing container # {container_no}\")\n",
    "            blog_post = extract_all_from_container(container)\n",
    "            blog_posts.append(blog_post)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Container # {container_no} not added: {str(e)}\")\n",
    "            pass\n",
    "    return blog_posts\n",
    "\n",
    "blog_posts = extract_all_from_containers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead28550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(blog_posts) != len(get_blog_containers())):\n",
    "    print(\"Not all containers could be transformed to blog_posts! No of conatiner: {len(containers)}, no of blog posts: {len(blog_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post_index = 1\n",
    "print(blog_posts[blog_post_index])\n",
    "#blog_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fd4f3",
   "metadata": {},
   "source": [
    "## Saving blog posts to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043dabc-f5f0-4e2f-aff5-93a5f6b89de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_text(some_text: str) -> str:\n",
    "    simplified_text = some_text.replace('\"', \"'\")\n",
    "    simplified_text = unidecode.unidecode(simplified_text)\n",
    "    simplified_text = re.sub(\"[^A-Za-z\\-_]+\", \"_\", simplified_text)\n",
    "    simplified_text = re.sub('_+', '_', simplified_text)\n",
    "    return simplified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403acb5-8677-4abf-a582-6333a403764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_title(blog_post):\n",
    "    LEN_OF_TITLE = 35\n",
    "    title = blog_post[\"text\"][:LEN_OF_TITLE].replace('\\n', ' ')\n",
    "    return title\n",
    "\n",
    "def build_title(blog_post):\n",
    "    LEN_OF_TITLE = 35\n",
    "    text = blog_post[\"text\"]\n",
    "    title = text[:LEN_OF_TITLE]\n",
    "    \n",
    "    if len(text) > LEN_OF_TITLE and text[LEN_OF_TITLE] != ' ':\n",
    "        # Extend to the end of the current word\n",
    "        while len(text) > len(title) and text[len(title)] != ' ':\n",
    "            title += text[len(title)]\n",
    "    \n",
    "    # Replace newlines with spaces in the final title\n",
    "    title = title.replace('\\n', ' ')\n",
    "    return title\n",
    "\n",
    "def build_simplified_title(blog_post: Dict) -> str:\n",
    "    simplified_title = simplify_text(build_title(blog_post))\n",
    "    return simplified_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filename(blog_post: Dict) -> str:\n",
    "    logger = get_logger(build_filename.__name__, logging.INFO)\n",
    "    LEN_OF_FILENAME = 45\n",
    "    posted_date = blog_post[\"posted_date\"]\n",
    "    try:\n",
    "        posted_date_for_filename = posted_date.strftime(INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        createdDateStrForFilename = \"_no_date_\"    \n",
    "    simplified_title = build_simplified_title(blog_post)[:LEN_OF_FILENAME-13]\n",
    "    filename = f\"{BLOGS_DIRECTORY}/{posted_date_for_filename}-{simplified_title}.md\"\n",
    "    logger.info(filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafa288-c918-41c6-9b18-e60a6e9ea893",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blog_post in blog_posts:\n",
    "    print(build_filename(blog_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0957fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frontmatter(blog_post):\n",
    "    posted_date = blog_post[\"posted_date\"]\n",
    "    title = build_title(blog_post)\n",
    "    original_url = blog_post[\"original_url\"]\n",
    "    frontMatter = (\"---\\n\"\n",
    "           \"layout: post\\n\"\n",
    "           \"date: \" + transformDate2String(posted_date) + \"\\n\"\n",
    "           'title: \"' + title + '\"\\n'\n",
    "           \"originalUrl: \\\"\" + original_url + \"\\\"\\n\")\n",
    "           #\"tags: linkedin \" + linkedin_user_based_tags + \"\\n\" +\n",
    "           #\"author: \\\"\" + author + \"\\\"\\n\")\n",
    "    frontMatter += \"---\\n\\n\"\n",
    "    return frontMatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_blog_post_to_file(blog_post: Dict) -> None:\n",
    "    content = blog_post[\"text\"]\n",
    "    filename = build_filename(blog_post)\n",
    "    frontmatter = build_frontmatter(blog_post)\n",
    "    path = os.path.dirname(filename)\n",
    "    #log(\"saveToFile\", \"Saving to file \", filename)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(frontmatter)\n",
    "        file.write(content)\n",
    "        file.close()\n",
    "\n",
    "def save_blog_posts_to_file(blog_posts):\n",
    "    for blog_post in blog_posts:\n",
    "        save_blog_post_to_file(blog_post)\n",
    "\n",
    "save_blog_posts_to_file(blog_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca29dc-b834-4597-83d0-8ef95dad6bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
