{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89eb7a18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Scraping-linkedin-Posts\" data-toc-modified-id=\"Scraping-linkedin-Posts-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Scraping linkedin Posts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Utils\" data-toc-modified-id=\"Utils-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Utils</a></span></li><li><span><a href=\"#Login\" data-toc-modified-id=\"Login-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Login</a></span></li><li><span><a href=\"#Load-posts-page-&amp;-scroll-to-bottom\" data-toc-modified-id=\"Load-posts-page-&amp;-scroll-to-bottom-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Load posts page &amp; scroll to bottom</a></span></li><li><span><a href=\"#Retrieve-data-from-loaded-page\" data-toc-modified-id=\"Retrieve-data-from-loaded-page-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Retrieve data from loaded page</a></span></li><li><span><a href=\"#Saving-blog-posts-to-files\" data-toc-modified-id=\"Saving-blog-posts-to-files-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Saving blog posts to files</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137062f",
   "metadata": {},
   "source": [
    "# Scraping linkedin Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5063146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from selenium import webdriver\n",
    "except:\n",
    "    %pip install selenium\n",
    "    from selenium import webdriver\n",
    "\n",
    "try:\n",
    "    import unidecode\n",
    "except:\n",
    "    %pip install unidecode\n",
    "    import unidecode\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    %pip install pandas\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc478841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re as re\n",
    "from importlib.metadata import version\n",
    "from typing import Any, Dict, Optional\n",
    "import os\n",
    "import types\n",
    "import logging\n",
    "import tkinter as tk\n",
    "import json\n",
    "from utils import getNowAsString, writeDictToFile, readDictFromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ba83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE = 'https://www.linkedin.com/company/mgm-technology-partners-gmbh'\n",
    "SCROLL_PAUSE_TIME = 1.5\n",
    "DATA_DIRECTORY = os.getenv('DATA_DIRECTORY') or 'data'\n",
    "os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
    "\n",
    "BLOGS_DIRECTORY = os.getenv('BLOGS_DIRECTORY') or f\"{DATA_DIRECTORY}/blogs\"\n",
    "os.makedirs(BLOGS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "TMP_DIRECTORY = os.getenv('TMP_DIRECTORY') or f\"{DATA_DIRECTORY}/tmp_linkedin\"\n",
    "os.makedirs(TMP_DIRECTORY, exist_ok=True)\n",
    "\n",
    "FILENAME_SOUP = \"linkedin_soup.html\"\n",
    "INTERNAL_DATE_FORMAT = \"%Y-%m-%d\"\n",
    "NO_DATE = \"__no_date__\"\n",
    "\n",
    "FILENAME_RAW_POSTS = f\"{TMP_DIRECTORY}/raw_posts.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f564b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f= open(\"credentials.txt\",\"r\")\n",
    "    contents = f.read()\n",
    "    username = contents.replace(\"=\",\",\").split(\",\")[1]\n",
    "    password = contents.replace(\"=\",\",\").split(\",\")[3]\n",
    "except:\n",
    "    f= open(\"credentials.txt\",\"w+\")\n",
    "    username = input('Enter your linkedin username: ')\n",
    "    password = input('Enter your linkedin password: ')\n",
    "    f.write(\"username={}, password={}\".format(username,password))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298c7d6",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e63214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name, log_level=logging.WARN):\n",
    "    # Get a logger with the given name\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.propagate = False  # Disable propagation to the root logger. Makes sense in Jupyter only...\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # Check if the logger has handlers already\n",
    "    if not logger.handlers:\n",
    "        # Create a handler\n",
    "        handler = logging.StreamHandler()\n",
    "\n",
    "        # Set a format that includes the logger's name\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "    \n",
    "def transformDate2String(dateToTransform: datetime) -> str:\n",
    "    logger = get_logger(transformDate2String.__name__)\n",
    "    try:\n",
    "        dateStr = dateToTransform.strftime(INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        logger.error(f\"Error transforming date: {dateToTransform}. Continuing with empty date string.\")\n",
    "        dateStr = \"\"\n",
    "    return dateStr\n",
    "\n",
    "def transformString2Date(stringToTransform: str) -> Optional[datetime]:\n",
    "    \"\"\"Transforms a String that holds a date in my standard format to a Date. \n",
    "        In case it can't transform it, it return None.\"\"\"\n",
    "    try:\n",
    "        dateObj = datetime.strptime(stringToTransform, INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        log(\"transformString2Date\", \"Error transforming string to date: \",\n",
    "            stringToTransform)\n",
    "        dateObj = None\n",
    "    return dateObj\n",
    "\n",
    "def getNowAsString() -> str:\n",
    "    return transformDate2String(datetime.now())\n",
    "\n",
    "def getMinDateAsString() -> str:\n",
    "    return transformDate2String(datetime(1970, 1, 1))\n",
    "\n",
    "def stripBlanks(str):\n",
    "    return str.strip(\" \\t\")\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20e1d0",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2850cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loggedin_browser():\n",
    "    #access Webriver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    #chrome_options.add_argument(\"--headless=new\")\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    #Open login page\n",
    "    browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "    \n",
    "    #Enter login info:\n",
    "    elementID = browser.find_element(by=By.ID, value='username')   #.find_element_by_id('username')\n",
    "    elementID.send_keys(username)\n",
    "    \n",
    "    elementID = browser.find_element(by=By.ID, value='password')#find_element_by_id('password')\n",
    "    elementID.send_keys(password)\n",
    "    #Note: replace the keys \"username\" and \"password\" with your LinkedIn login info\n",
    "    elementID.submit()\n",
    "    return browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc679d3",
   "metadata": {},
   "source": [
    "## Load posts page & scroll to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea79bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def browser_go_to_page(browser, max_pages=0):\n",
    "    logger = get_logger(browser_go_to_page.__name__, logging.INFO)\n",
    "    #Go to webpage\n",
    "    company_posts_page = PAGE + '/posts/'\n",
    "    logger.info(f\"{company_posts_page=}\")\n",
    "    browser.get(company_posts_page)\n",
    "    \n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_page = 0\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        #click_visible_menues(browser)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        scroll_page += 1\n",
    "        logger.info(f\"Scrolling page {scroll_page}\")\n",
    "        \n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "    \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        if max_pages > 0:\n",
    "            if scroll_page == max_pages:\n",
    "                break\n",
    "                \n",
    "    return \n",
    "\n",
    "def get_page_source(browser, max_pages=0):\n",
    "    logger = get_logger(get_page_source.__name__, logging.INFO)\n",
    "    browser_go_to_page(browser, max_pages)\n",
    "\n",
    "    company_page = browser.page_source   \n",
    "    return company_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0365b2ff-716a-4545-a49b-256fcdf35efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkedin_browser(max_pages=0):\n",
    "    browser = get_loggedin_browser()\n",
    "    browser_go_to_page(browser, max_pages=max_pages)\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72975e00",
   "metadata": {},
   "source": [
    "## Retrieve data from loaded page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ce246d-efb0-4c4c-931c-37ecaff6377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_container_elements(max_pages):\n",
    "    logger = get_logger(get_container_elements.__name__, logging.INFO)\n",
    "    browser = get_linkedin_browser(max_pages=max_pages)\n",
    "    container_elements = browser.find_elements(By.CLASS_NAME, \"occludable-update\")\n",
    "    logger.info(f\"No of container elements before filter: {len(container_elements)}\")\n",
    "    container_elements = [element for element in container_elements if len(element.find_elements(By.CLASS_NAME,\"update-components-actor\")) > 0]\n",
    "    logger.info(f\"No of container elements after filter: {len(container_elements)}\")\n",
    "    return container_elements, browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eeb008c-dc2a-426d-99ae-fa3bf1af7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_element_in_viewport(driver, element):\n",
    "    return driver.execute_script(\"\"\"\n",
    "        var elem = arguments[0];\n",
    "        var rect = elem.getBoundingClientRect();\n",
    "        return (\n",
    "            rect.top >= 0 &&\n",
    "            rect.left >= 0 &&\n",
    "            rect.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&\n",
    "            rect.right <= (window.innerWidth || document.documentElement.clientWidth)\n",
    "        );\n",
    "    \"\"\", element)\n",
    "\n",
    "def get_post_url(browser):\n",
    "    logger = get_logger(get_post_url.__name__, logging.WARN)\n",
    "    elements = browser.find_elements(By.XPATH, \"//*[text()='Copy link to post']\")\n",
    "    if len(elements) != 1:\n",
    "        logger.warning(f\"Number of list of elements that should give me the URL of the blogpost: {len(elements)}\")\n",
    "        return None\n",
    "    try:\n",
    "        elements[0].click()\n",
    "        root = tk.Tk()\n",
    "        blog_post_url = root.clipboard_get()\n",
    "        logger.info(f\"URL of blog post: {blog_post_url}\")\n",
    "        return blog_post_url\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"Could not extract blog post url, retrurning None. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_blog_post_url_from_container_element(browser, container_element):\n",
    "    logger = get_logger(extract_blog_post_url_from_container_element.__name__, logging.INFO)\n",
    "    #logger.info(f\"Extracting from container of type {type(container_element)}\")\n",
    "    buttons = container_element.find_elements(By.CLASS_NAME, 'feed-shared-control-menu__trigger')  \n",
    "    if len(buttons) != 1:\n",
    "        logger.info(f\"No of buttons found in container: {len(buttons)}. Cannot process this container.\")\n",
    "        return None\n",
    "        \n",
    "    button = buttons[0]\n",
    "    actions = ActionChains(browser)\n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    browser.execute_script('arguments[0].scrollIntoView({ behavior: \"smooth\", block: \"center\", inline: \"nearest\" });', button)\n",
    "    \n",
    "    if not button.is_displayed():  \n",
    "        logger.warn(\"Button not displayed, cannot process container\")\n",
    "        return None\n",
    "        \n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    time.sleep(1)  \n",
    "    button.click()\n",
    "    time.sleep(5)  \n",
    "    url = get_post_url(browser)                \n",
    "    actions.send_keys(Keys.ESCAPE).perform()\n",
    "    return url\n",
    "    \n",
    "def write_blog_containers_to_file(blogs):\n",
    "    logger = get_logger(write_blog_containers_to_file.__name__, logging.INFO)\n",
    "    # Prepare blogs to be saveable i.e. serializable\n",
    "    blogs_to_save = []\n",
    "    for blog in blogs:\n",
    "        blog_to_save = blog\n",
    "        blog_to_save[\"soup\"] = blog[\"soup\"].prettify()\n",
    "        blogs_to_save.append(blog_to_save)\n",
    "    try:\n",
    "        f= open(FILENAME_RAW_POSTS,\"w+\")\n",
    "        json.dump(blogs_to_save, f)\n",
    "        f.close()\n",
    "        logger.info(f\"Wrote {len(blogs)} blog containers to file {FILENAME_RAW_POSTS}.\")\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"could not write {len(blogs)} blog containers to file {FILENAME_RAW_POSTS}: {e}\")        \n",
    "    return \n",
    "\n",
    "def write_blog_containers_to_file(blogs):\n",
    "    logger = get_logger(write_blog_containers_to_file.__name__, logging.INFO)\n",
    "    # Prepare blogs to be saveable i.e. serializable\n",
    "    blogs_to_save = []\n",
    "    for blog in blogs:\n",
    "        blog_to_save = blog\n",
    "        blog_to_save[\"soup\"] = blog[\"soup\"].prettify()\n",
    "        blogs_to_save.append(blog_to_save)\n",
    "    try:\n",
    "        writeDictToFile(dictionary=blogs_to_save,fullFilename=FILENAME_RAW_POSTS)\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"could not write {len(blogs)} blog containers to file {FILENAME_RAW_POSTS}: {e}\")        \n",
    "    return \n",
    "\n",
    "\n",
    "def read_blog_containers_from_file():\n",
    "    logger = get_logger(read_blog_containers_from_file.__name__, logging.INFO)\n",
    "    blogs = []\n",
    "    try:\n",
    "        f= open(FILENAME_RAW_POSTS,\"r+\")\n",
    "        blogs = json.load(f)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"Could not read blog containers from file {FILENAME_RAW_POSTS}. Returning None.\")\n",
    "        return None\n",
    "    for blog in blogs:\n",
    "        blog[\"soup\"] = bs(blog[\"soup\"], \"html.parser\")  # convert string to BeautifulSoup object\n",
    "    logger.info(f\"Read {len(blogs)} blog containersc from file {FILENAME_RAW_POSTS}.\")\n",
    "    return blogs\n",
    "\n",
    "\n",
    "def read_blog_containers_from_file():\n",
    "    logger = get_logger(read_blog_containers_from_file.__name__, logging.INFO)\n",
    "    blogs = []\n",
    "    try:\n",
    "        blogs = readDictFromFile(fullFilename=FILENAME_RAW_POSTS)\n",
    "    except Exception as e:\n",
    "        logger.warn(f\"Could not read blog containers from file {FILENAME_RAW_POSTS}. Returning None.\")\n",
    "        return None\n",
    "    for blog in blogs:\n",
    "        blog[\"soup\"] = bs(blog[\"soup\"], \"html.parser\")  # convert string to BeautifulSoup object\n",
    "    logger.info(f\"Read {len(blogs)} blog containersc from file {FILENAME_RAW_POSTS}.\")\n",
    "    return blogs\n",
    "\n",
    "def extract_blogs_from_container_elements(browser, container_elements):\n",
    "    logger = get_logger(extract_blogs_from_container_elements.__name__, logging.INFO)\n",
    "    blogs = []\n",
    "    for container_element in container_elements:\n",
    "        blog_url = extract_blog_post_url_from_container_element(browser, container_element)\n",
    "        blog_source = container_element.get_attribute('outerHTML')\n",
    "        blog_soup = bs(blog_source.encode(\"utf-8\"), \"html\")\n",
    "        blog = {\n",
    "            \"url\": blog_url,\n",
    "            \"source\": blog_source,\n",
    "            \"soup\": blog_soup,\n",
    "            \"scrape_date\": getNowAsString()\n",
    "        }\n",
    "        blogs.append(blog)\n",
    "                         \n",
    "    logger.info(f\"No of extracted blogs: {len(blogs)}\")\n",
    "    write_blog_containers_to_file(blogs)\n",
    "    return blogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427c0b94-1dd1-4054-b1f6-3356b0e115b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/p4nc2g_90rn8wht95hn0m3lw0000gn/T/ipykernel_51316/1377201222.py:108: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(f\"Could not read blog containers from file {FILENAME_RAW_POSTS}. Returning None.\")\n",
      "2023-09-04 07:53:10,266 - read_blog_containers_from_file - WARNING - Could not read blog containers from file data/tmp_linkedin/raw_posts.json. Returning None.\n"
     ]
    }
   ],
   "source": [
    "def get_blog_containers(force_retrieval=False, max_pages=0):\n",
    "    logger = get_logger(get_blog_containers.__name__, logging.INFO)\n",
    "    if force_retrieval:\n",
    "        logger.info(f\"Retrieving blog containers: {force_retrieval=} {max_pages=}\")\n",
    "        container_elements, browser = retrieve_container_elements(max_pages)\n",
    "        blog_containers = extract_blogs_from_container_elements(browser, container_elements)   \n",
    "        return blog_containers\n",
    "    try:\n",
    "        blog_containers = read_blog_containers_from_file()\n",
    "        return blog_containers\n",
    "    except Exception as e:\n",
    "        container_elements, browser = retrieve_container_elements(max_pages)\n",
    "        blog_containers = extract_blogs_from_container_elements(browser, container_elements)   \n",
    "        return blog_containers\n",
    "\n",
    "blog_container = get_blog_containers(force_retrieval=False, max_pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0c10cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO Delete these functions\n",
    "\n",
    "def get_linkedin_soup_from_website(max_pages=0):\n",
    "    logger = get_logger(get_linkedin_soup_from_website.__name__, logging.INFO)\n",
    "    browser = get_loggedin_browser()\n",
    "    company_page = get_page_source(browser, max_pages)\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    f= open(FILENAME_SOUP,\"w+\")\n",
    "    f.write(linkedin_soup.prettify())\n",
    "    f.close()\n",
    "    logger.info(\"Scraped soup from website\")\n",
    "    return linkedin_soup, browser\n",
    "\n",
    "def get_linkedin_soup_from_file():\n",
    "    logger = get_logger(get_linkedin_soup_from_file.__name__, logging.INFO)\n",
    "    f= open(FILENAME_SOUP,\"r+\")\n",
    "    linkedin_html = f.read()\n",
    "    f.close()\n",
    "    linkedin_soup = bs(linkedin_html, \"html.parser\")  # convert string to BeautifulSoup object\n",
    "    logger.info(\"Read soup from file\")\n",
    "    return linkedin_soup\n",
    "\n",
    "def get_linkedin_soup(force_retrieval, max_pages):\n",
    "    logger = get_logger(get_linkedin_soup.__name__, logging.INFO)\n",
    "    browser = None\n",
    "    if force_retrieval:\n",
    "        logger.info(f\"Retrieving linkedin soup: {force_retrieval=} {max_pages=}\")\n",
    "        linkedin_soup, browser = get_linkedin_soup_from_website(max_pages=max_pages)\n",
    "        return linkedin_soup, browser\n",
    "        \n",
    "    try:\n",
    "        linkedin_soup = get_linkedin_soup_from_file()\n",
    "    except:\n",
    "        linkedin_soup = get_linked_soup_from_website(max_pages=max_pages)\n",
    "    return linkedin_soup, browser\n",
    "    \n",
    "#linkedin_soup = get_linkedin_soup(force_retrieval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b47d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Delete these functions\n",
    "\n",
    "def get_containers(force_retrieval=False, max_pages=0):\n",
    "    logger = get_logger(get_containers.__name__, logging.INFO)\n",
    "    \n",
    "    linkedin_soup, browser = get_linkedin_soup(force_retrieval=force_retrieval, max_pages=max_pages)\n",
    "    containers = linkedin_soup.find_all(class_=\"occludable-update\")\n",
    "    \n",
    "    # We also need to check that the conatiner contains \"update-components-actor\" to filter out ads\n",
    "    containers = [container for container in containers if container.find(class_=\"update-components-actor\")]    \n",
    "    logger.info(f\"Number of container: {len(containers)}\")\n",
    "\n",
    "    if not browser is None:\n",
    "        for container in containers:\n",
    "            blog_url = extract_blog_post_url_from_container(container, browser)\n",
    "            \n",
    "    # Write it to disk for analyzing\n",
    "    containers_to_write = [0,1,2,3, 5]\n",
    "    for container_no in containers_to_write:\n",
    "        if len(containers) > container_no:\n",
    "            filename = f\"{TMP_DIRECTORY}/container_{container_no}.html\" \n",
    "            logger.info(f\"Writing container {container_no} to {filename}\")\n",
    "            f= open(filename,\"w+\")\n",
    "            f.write(containers[container_no].prettify())\n",
    "            f.close()\n",
    "\n",
    "    return containers, browser\n",
    "    \n",
    "#containers, browser = get_containers(force_retrieval=True, max_pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f7e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 07:36:34,125 - read_blog_containers_from_file - INFO - Read 22 blog containersc from file data/tmp_linkedin/raw_posts.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23h •\n"
     ]
    }
   ],
   "source": [
    "def extract_date_string_from_soup(soup: bs):\n",
    "    logger = get_logger(extract_date_string_from_soup.__name__, log_level=logging.WARN)\n",
    "\n",
    "    # Looking for the relative date (in d, w, mo, yr)\n",
    "    # It has the shape: \"1yr •\"\n",
    "    p = re.compile(r'\\d{1,2}(h|d|w|mo|yr)\\s•')\n",
    "    m = re.compile(r'\\d{1,2}(h|d|w|mo|yr)\\s•').search(soup.prettify())\n",
    "    dateHumanReadable = \"\"\n",
    "    if m:\n",
    "        dateHumanReadable = m.group()\n",
    "        logger.info(f\"Match found: {dateHumanReadable}\")\n",
    "        return dateHumanReadable\n",
    "    else:\n",
    "        logger.error(f\"Could not extract human readable date from soup! soup: {soup}\")\n",
    "        return NO_DATE\n",
    "\n",
    "def test_extract_date_string_from_soup():\n",
    "    containers = get_blog_containers()\n",
    "    human_readable_date = extract_date_string_from_soup(containers[0][\"soup\"])\n",
    "    print(human_readable_date)\n",
    "\n",
    "test_extract_date_string_from_soup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ced8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2h • --> 2023-09-04 05:36:34.139794\n",
      "3d • --> 2023-09-01 07:36:34.139962\n",
      "1w • --> 2023-08-28 07:36:34.140006\n"
     ]
    }
   ],
   "source": [
    "def linkedin_rel_date2datetime(relative_date):\n",
    "    \"\"\"Transforms a relative date from LinkedIn to a datetime object.\n",
    "    Transform \"6d •\" to a proper datetime\"\"\"\n",
    "\n",
    "    logger = get_logger(linkedin_rel_date2datetime.__name__, log_level=logging.WARN)\n",
    "    \n",
    "    p = re.compile('\\d{1,2}')\n",
    "    m = p.search(relative_date)\n",
    "    if m is None:\n",
    "        logger.error(f\"Amount not found in {relative_date}\")\n",
    "        exit\n",
    "    amount = float(m.group())\n",
    "    p = re.compile('(h|d|w|mo|yr)')\n",
    "    m = p.search(relative_date)\n",
    "    logger.info(f\"m: {m}, type(m): {type(m)}\")\n",
    "    if m is None:\n",
    "        logger.error(f\"Unit not found in {relative_date}\")\n",
    "        exit\n",
    "    unit = m.group()\n",
    "    if unit == 'yr':\n",
    "        amount *= 365*24\n",
    "    elif unit == 'mo':\n",
    "        amount *= 30*24\n",
    "    elif unit == 'w':\n",
    "        amount *= 7*24\n",
    "    elif unit == 'd':\n",
    "        amount *= 24\n",
    "    logger.info(f\" {relative_date} --> Amount in hours: {amount}\")\n",
    "    # Calculate the date from today's, and return it\n",
    "    howRecent = timedelta(hours=amount)\n",
    "    todaysDate = datetime.now()\n",
    "    date = (todaysDate - howRecent)\n",
    "    return date\n",
    "\n",
    "# Some tests\n",
    "rel_dates = ['2h •', '3d •', '1w •']\n",
    "for rel_date in rel_dates:\n",
    "    print(f\"{rel_date} --> {linkedin_rel_date2datetime(rel_date)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2a89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_soup(soup: bs):\n",
    "    logger = get_logger(extract_text_from_soup.__name__, log_level=logging.INFO)\n",
    "\n",
    "    # In 'container', find the first <div> element with class 'feed-shared-update-v2__description-wrapper'.\n",
    "    # Assign this element to 'text_box'.\n",
    "    text_box = soup.find(\"div\", {\"class\":\"feed-shared-update-v2__description-wrapper\"})\n",
    "    \n",
    "    # If 'text_box' is not None (i.e., if such an element was found in 'container')...\n",
    "    if text_box:\n",
    "        # ...find the first <span> element within 'text_box' that has the 'dir' attribute set to 'ltr'.\n",
    "        # Extract its text content, strip leading and trailing whitespace, and assign this cleaned text to 'text'.\n",
    "        text = text_box.find(\"span\", {\"dir\":\"ltr\"}).text.strip()\n",
    "        \n",
    "        # Return 'text'.\n",
    "        return text\n",
    "    else:\n",
    "        # If 'text_box' is None (i.e., if no such <div> element was found in 'container')...\n",
    "        # ...print an error message.\n",
    "        logger.warn(f\"Could not extract text from soup!\")\n",
    "        \n",
    "        # Uncomment the following line to print the 'container' for debugging purposes.\n",
    "        # print(f\"Container: {container}\")\n",
    "        \n",
    "        # Return an empty string.\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d1bca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 07:36:34,442 - read_blog_containers_from_file - INFO - Read 22 blog containersc from file data/tmp_linkedin/raw_posts.json.\n",
      "2023-09-04 07:36:34,442 - extract_all_from_containers - INFO - Processing container # 0\n",
      "2023-09-04 07:36:34,449 - extract_all_from_container - INFO - 2023-09-03 08:36:34.448883 - Der Einsatz von Modellen in de\n",
      "2023-09-04 07:36:34,450 - extract_all_from_containers - INFO - Processing container # 1\n",
      "2023-09-04 07:36:34,457 - extract_all_from_container - INFO - 2023-08-31 07:36:34.457391 - Skalierung in der Softwareentw\n",
      "2023-09-04 07:36:34,458 - extract_all_from_containers - INFO - Processing container # 2\n",
      "2023-09-04 07:36:34,466 - extract_all_from_container - INFO - 2023-08-14 07:36:34.465461 - „Gut getestet aber auch sicher\n",
      "2023-09-04 07:36:34,466 - extract_all_from_containers - INFO - Processing container # 3\n",
      "2023-09-04 07:36:34,473 - extract_all_from_container - INFO - 2023-08-21 07:36:34.473167 - Wesen und Stärke von\n",
      "\n",
      "#OpenSou\n",
      "2023-09-04 07:36:34,474 - extract_all_from_containers - INFO - Processing container # 4\n",
      "2023-09-04 07:36:34,480 - extract_all_from_container - INFO - 2023-08-29 07:36:34.480170 - Enterprise Low Code-Plattforme\n",
      "2023-09-04 07:36:34,481 - extract_all_from_containers - INFO - Processing container # 5\n",
      "2023-09-04 07:36:34,488 - extract_all_from_container - INFO - 2023-08-28 07:36:34.487997 - Für unsere A12 Low Code-Plattf\n",
      "2023-09-04 07:36:34,489 - extract_all_from_containers - INFO - Processing container # 6\n",
      "2023-09-04 07:36:34,495 - extract_all_from_container - INFO - 2023-08-28 07:36:34.495487 - In einer Software-getriebenen \n",
      "2023-09-04 07:36:34,496 - extract_all_from_containers - INFO - Processing container # 7\n",
      "2023-09-04 07:36:34,503 - extract_all_from_container - INFO - 2023-08-28 07:36:34.503387 - Die modellbasierte Softwareent\n",
      "2023-09-04 07:36:34,504 - extract_all_from_containers - INFO - Processing container # 8\n",
      "2023-09-04 07:36:34,511 - extract_all_from_container - INFO - 2023-08-28 07:36:34.511101 - Die Offenheit der mgm A12 Low-\n",
      "2023-09-04 07:36:34,512 - extract_all_from_containers - INFO - Processing container # 9\n",
      "2023-09-04 07:36:34,520 - extract_all_from_container - INFO - 2023-08-28 07:36:34.519515 - Bei der Sicherheitsplanung in \n",
      "2023-09-04 07:36:34,520 - extract_all_from_containers - INFO - Processing container # 10\n",
      "2023-09-04 07:36:34,528 - extract_all_from_container - INFO - 2023-08-21 07:36:34.527615 - Programmierung ist ein wichtig\n",
      "2023-09-04 07:36:34,528 - extract_all_from_containers - INFO - Processing container # 11\n",
      "2023-09-04 07:36:34,537 - extract_all_from_container - INFO - 2023-08-21 07:36:34.536614 - Die Globalisierung hat in den \n",
      "2023-09-04 07:36:34,537 - extract_all_from_containers - INFO - Processing container # 12\n",
      "2023-09-04 07:36:34,544 - extract_all_from_container - INFO - 2023-08-28 07:36:34.544325 - Um die nationalen und internat\n",
      "2023-09-04 07:36:34,545 - extract_all_from_containers - INFO - Processing container # 13\n",
      "2023-09-04 07:36:34,553 - extract_all_from_container - INFO - 2023-08-05 07:36:34.552550 - Die Bedeutung einer nahtlosen \n",
      "2023-09-04 07:36:34,553 - extract_all_from_containers - INFO - Processing container # 14\n",
      "2023-09-04 07:36:34,561 - extract_all_from_container - INFO - 2023-08-05 07:36:34.560747 - In der neuen Ausgabe von\n",
      "\n",
      "DIE \n",
      "2023-09-04 07:36:34,561 - extract_all_from_containers - INFO - Processing container # 15\n",
      "2023-09-04 07:36:34,569 - extract_all_from_container - INFO - 2023-08-21 07:36:34.568788 - Heute Grenoble, morgen Hamburg\n",
      "2023-09-04 07:36:34,569 - extract_all_from_containers - INFO - Processing container # 16\n",
      "2023-09-04 07:36:34,576 - extract_all_from_container - INFO - 2023-08-21 07:36:34.576055 - Der Bedarf an schnellen digita\n",
      "2023-09-04 07:36:34,577 - extract_all_from_containers - INFO - Processing container # 17\n",
      "2023-09-04 07:36:34,584 - extract_all_from_container - INFO - 2023-07-06 07:36:34.583852 - This weekend the Porto team go\n",
      "2023-09-04 07:36:34,585 - extract_all_from_containers - INFO - Processing container # 18\n",
      "2023-09-04 07:36:34,591 - extract_all_from_container - INFO - 2023-07-06 07:36:34.591347 - Dicke Bretter nochmals zum Sch\n",
      "2023-09-04 07:36:34,592 - extract_all_from_containers - INFO - Processing container # 19\n",
      "2023-09-04 07:36:34,599 - extract_all_from_container - INFO - 2023-08-21 07:36:34.599326 - Nachhaltige Softwareprojekte k\n",
      "2023-09-04 07:36:34,600 - extract_all_from_containers - INFO - Processing container # 20\n",
      "2023-09-04 07:36:34,607 - extract_all_from_container - INFO - 2023-07-06 07:36:34.607262 - Wir freuen uns auf die\n",
      "\n",
      "K5\n",
      "\n",
      "Fu\n",
      "2023-09-04 07:36:34,608 - extract_all_from_containers - INFO - Processing container # 21\n",
      "2023-09-04 07:36:34,616 - extract_all_from_container - INFO - 2023-08-21 07:36:34.615484 - Die Fähigkeit auf Veränderunge\n"
     ]
    }
   ],
   "source": [
    "def simplify_content(content):\n",
    "    content = re.sub('\\n +', '\\n', content)\n",
    "    content = re.sub('\\n+', '\\n\\n', content)\n",
    "    content = content.replace(\"{\", \"&#123;\").replace(\"}\", \"&#125;\")\n",
    "    return content\n",
    "    \n",
    "def extract_all_from_container(container):\n",
    "    logger = get_logger(extract_all_from_container.__name__, logging.INFO)\n",
    "    blog_post = {}\n",
    "    blog_post[\"date_human_readable\"] = extract_date_string_from_soup(container[\"soup\"])\n",
    "    blog_post[\"posted_date\"] = linkedin_rel_date2datetime(blog_post[\"date_human_readable\"])\n",
    "    blog_post[\"text\"] = simplify_content(extract_text_from_soup(container[\"soup\"]))\n",
    "    blog_post[\"original_url\"] = container[\"url\"]\n",
    "    logger.info(f\"{blog_post['posted_date']} - {blog_post['text'][:30]}\")\n",
    "    return blog_post\n",
    "\n",
    "def extract_all_from_containers():\n",
    "    logger = get_logger(extract_all_from_containers.__name__, logging.INFO)\n",
    "    containers = get_blog_containers()\n",
    "    blog_posts = []\n",
    "    \n",
    "    for container_no, container in enumerate(containers):\n",
    "        try:\n",
    "            logger.info(f\"Processing container # {container_no}\")\n",
    "            blog_post = extract_all_from_container(container)\n",
    "            blog_posts.append(blog_post)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Container # {container_no} not added: {str(e)}\")\n",
    "            pass\n",
    "    return blog_posts\n",
    "\n",
    "blog_posts = extract_all_from_containers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead28550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 07:36:34,835 - read_blog_containers_from_file - INFO - Read 22 blog containersc from file data/tmp_linkedin/raw_posts.json.\n"
     ]
    }
   ],
   "source": [
    "if (len(blog_posts) != len(get_blog_containers())):\n",
    "    print(\"Not all containers could be transformed to blog_posts! No of conatiner: {len(containers)}, no of blog posts: {len(blog_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77efac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date_human_readable': '4d •', 'posted_date': datetime.datetime(2023, 8, 31, 7, 36, 34, 457391), 'text': 'Skalierung in der Softwareentwicklung bezieht sich auf die Projektgröße und die Rolle von Einzelpersonen im Laufe der Zeit.\\n\\nDie Zusammenarbeit in Teams birgt neue Herausforderungen, ermöglicht aber auch die Entwicklung nachhaltigerer und wertvollerer Systeme.\\n\\nLesen Sie mehr dazu in unserem Blogbeitrag:\\n\\n➡️\\n\\nhttps://lnkd.in/gpKBJp2z\\n\\n#softwareengineering\\n\\n#programmierung\\n\\n#enterprisesoftwareengineering\\n\\n#mgmtechnologypartners', 'original_url': 'https://www.linkedin.com/posts/mgm-technology-partners-gmbh_die-prinzipien-des-enterprise-software-engineerings-activity-7097879746960596992-pMtH?utm_source=share&utm_medium=member_desktop'}\n"
     ]
    }
   ],
   "source": [
    "blog_post_index = 1\n",
    "print(blog_posts[blog_post_index])\n",
    "#blog_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fd4f3",
   "metadata": {},
   "source": [
    "## Saving blog posts to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9043dabc-f5f0-4e2f-aff5-93a5f6b89de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_text(some_text: str) -> str:\n",
    "    simplified_text = some_text.replace('\"', \"'\")\n",
    "    simplified_text = unidecode.unidecode(simplified_text)\n",
    "    simplified_text = re.sub(\"[^A-Za-z\\-_]+\", \"_\", simplified_text)\n",
    "    simplified_text = re.sub('_+', '_', simplified_text)\n",
    "    return simplified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7403acb5-8677-4abf-a582-6333a403764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_title(blog_post):\n",
    "    LEN_OF_TITLE = 35\n",
    "    title = blog_post[\"text\"][:LEN_OF_TITLE].replace('\\n', ' ')\n",
    "    return title\n",
    "\n",
    "def build_title(blog_post):\n",
    "    LEN_OF_TITLE = 35\n",
    "    text = blog_post[\"text\"]\n",
    "    title = text[:LEN_OF_TITLE]\n",
    "    \n",
    "    if len(text) > LEN_OF_TITLE and text[LEN_OF_TITLE] != ' ':\n",
    "        # Extend to the end of the current word\n",
    "        while len(text) > len(title) and text[len(title)] != ' ':\n",
    "            title += text[len(title)]\n",
    "    \n",
    "    # Replace newlines with spaces in the final title\n",
    "    title = title.replace('\\n', ' ')\n",
    "    return title\n",
    "\n",
    "def build_simplified_title(blog_post: Dict) -> str:\n",
    "    simplified_title = simplify_text(build_title(blog_post))\n",
    "    return simplified_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52cb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filename(blog_post: Dict) -> str:\n",
    "    logger = get_logger(build_filename.__name__, logging.INFO)\n",
    "    LEN_OF_FILENAME = 45\n",
    "    posted_date = blog_post[\"posted_date\"]\n",
    "    try:\n",
    "        posted_date_for_filename = posted_date.strftime(INTERNAL_DATE_FORMAT)\n",
    "    except:\n",
    "        createdDateStrForFilename = \"_no_date_\"    \n",
    "    simplified_title = build_simplified_title(blog_post)[:LEN_OF_FILENAME-13]\n",
    "    filename = f\"{BLOGS_DIRECTORY}/{posted_date_for_filename}-{simplified_title}.md\"\n",
    "    logger.info(filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eafa288-c918-41c6-9b18-e60a6e9ea893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 07:36:34,866 - build_filename - INFO - data/blogs/2023-09-03-Der_Einsatz_von_Modellen_in_der_.md\n",
      "2023-09-04 07:36:34,867 - build_filename - INFO - data/blogs/2023-08-31-Skalierung_in_der_Softwareentwic.md\n",
      "2023-09-04 07:36:34,872 - build_filename - INFO - data/blogs/2023-08-14-_Gut_getestet_aber_auch_sicher_i.md\n",
      "2023-09-04 07:36:34,874 - build_filename - INFO - data/blogs/2023-08-21-Wesen_und_Starke_von_OpenSourceS.md\n",
      "2023-09-04 07:36:34,875 - build_filename - INFO - data/blogs/2023-08-29-Enterprise_Low_Code-Plattformen_.md\n",
      "2023-09-04 07:36:34,876 - build_filename - INFO - data/blogs/2023-08-28-Fur_unsere_A_Low_Code-Plattform_.md\n",
      "2023-09-04 07:36:34,876 - build_filename - INFO - data/blogs/2023-08-28-In_einer_Software-getriebenen_Or.md\n",
      "2023-09-04 07:36:34,877 - build_filename - INFO - data/blogs/2023-08-28-Die_modellbasierte_Softwareentwi.md\n",
      "2023-09-04 07:36:34,877 - build_filename - INFO - data/blogs/2023-08-28-Die_Offenheit_der_mgm_A_Low-Code.md\n",
      "2023-09-04 07:36:34,878 - build_filename - INFO - data/blogs/2023-08-28-Bei_der_Sicherheitsplanung_in_ei.md\n",
      "2023-09-04 07:36:34,878 - build_filename - INFO - data/blogs/2023-08-21-Programmierung_ist_ein_wichtiger.md\n",
      "2023-09-04 07:36:34,879 - build_filename - INFO - data/blogs/2023-08-21-Die_Globalisierung_hat_in_den_le.md\n",
      "2023-09-04 07:36:34,880 - build_filename - INFO - data/blogs/2023-08-28-Um_die_nationalen_und_internatio.md\n",
      "2023-09-04 07:36:34,880 - build_filename - INFO - data/blogs/2023-08-05-Die_Bedeutung_einer_nahtlosen_Cu.md\n",
      "2023-09-04 07:36:34,881 - build_filename - INFO - data/blogs/2023-08-05-In_der_neuen_Ausgabe_von_DIE_MAC.md\n",
      "2023-09-04 07:36:34,881 - build_filename - INFO - data/blogs/2023-08-21-Heute_Grenoble_morgen_Hamburg_Da.md\n",
      "2023-09-04 07:36:34,881 - build_filename - INFO - data/blogs/2023-08-21-Der_Bedarf_an_schnellen_digitale.md\n",
      "2023-09-04 07:36:34,882 - build_filename - INFO - data/blogs/2023-07-06-This_weekend_the_Porto_team_got_.md\n",
      "2023-09-04 07:36:34,882 - build_filename - INFO - data/blogs/2023-07-06-Dicke_Bretter_nochmals_zum_Schlu.md\n",
      "2023-09-04 07:36:34,883 - build_filename - INFO - data/blogs/2023-08-21-Nachhaltige_Softwareprojekte_kon.md\n",
      "2023-09-04 07:36:34,883 - build_filename - INFO - data/blogs/2023-07-06-Wir_freuen_uns_auf_die_K_Future_.md\n",
      "2023-09-04 07:36:34,884 - build_filename - INFO - data/blogs/2023-08-21-Die_Fahigkeit_auf_Veranderungen_.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/blogs/2023-09-03-Der_Einsatz_von_Modellen_in_der_.md\n",
      "data/blogs/2023-08-31-Skalierung_in_der_Softwareentwic.md\n",
      "data/blogs/2023-08-14-_Gut_getestet_aber_auch_sicher_i.md\n",
      "data/blogs/2023-08-21-Wesen_und_Starke_von_OpenSourceS.md\n",
      "data/blogs/2023-08-29-Enterprise_Low_Code-Plattformen_.md\n",
      "data/blogs/2023-08-28-Fur_unsere_A_Low_Code-Plattform_.md\n",
      "data/blogs/2023-08-28-In_einer_Software-getriebenen_Or.md\n",
      "data/blogs/2023-08-28-Die_modellbasierte_Softwareentwi.md\n",
      "data/blogs/2023-08-28-Die_Offenheit_der_mgm_A_Low-Code.md\n",
      "data/blogs/2023-08-28-Bei_der_Sicherheitsplanung_in_ei.md\n",
      "data/blogs/2023-08-21-Programmierung_ist_ein_wichtiger.md\n",
      "data/blogs/2023-08-21-Die_Globalisierung_hat_in_den_le.md\n",
      "data/blogs/2023-08-28-Um_die_nationalen_und_internatio.md\n",
      "data/blogs/2023-08-05-Die_Bedeutung_einer_nahtlosen_Cu.md\n",
      "data/blogs/2023-08-05-In_der_neuen_Ausgabe_von_DIE_MAC.md\n",
      "data/blogs/2023-08-21-Heute_Grenoble_morgen_Hamburg_Da.md\n",
      "data/blogs/2023-08-21-Der_Bedarf_an_schnellen_digitale.md\n",
      "data/blogs/2023-07-06-This_weekend_the_Porto_team_got_.md\n",
      "data/blogs/2023-07-06-Dicke_Bretter_nochmals_zum_Schlu.md\n",
      "data/blogs/2023-08-21-Nachhaltige_Softwareprojekte_kon.md\n",
      "data/blogs/2023-07-06-Wir_freuen_uns_auf_die_K_Future_.md\n",
      "data/blogs/2023-08-21-Die_Fahigkeit_auf_Veranderungen_.md\n"
     ]
    }
   ],
   "source": [
    "for blog_post in blog_posts:\n",
    "    print(build_filename(blog_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0957fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frontmatter(blog_post):\n",
    "    posted_date = blog_post[\"posted_date\"]\n",
    "    title = build_title(blog_post)\n",
    "    original_url = blog_post[\"original_url\"]\n",
    "    frontMatter = (\"---\\n\"\n",
    "           \"layout: post\\n\"\n",
    "           \"date: \" + transformDate2String(posted_date) + \"\\n\"\n",
    "           'title: \"' + title + '\"\\n'\n",
    "           \"originalUrl: \\\"\" + original_url + \"\\\"\\n\")\n",
    "           #\"tags: linkedin \" + linkedin_user_based_tags + \"\\n\" +\n",
    "           #\"author: \\\"\" + author + \"\\\"\\n\")\n",
    "    frontMatter += \"---\\n\\n\"\n",
    "    return frontMatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e3a8bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 07:36:34,896 - build_filename - INFO - data/blogs/2023-09-03-Der_Einsatz_von_Modellen_in_der_.md\n",
      "2023-09-04 07:36:34,897 - build_filename - INFO - data/blogs/2023-08-31-Skalierung_in_der_Softwareentwic.md\n",
      "2023-09-04 07:36:34,899 - build_filename - INFO - data/blogs/2023-08-14-_Gut_getestet_aber_auch_sicher_i.md\n",
      "2023-09-04 07:36:34,901 - build_filename - INFO - data/blogs/2023-08-21-Wesen_und_Starke_von_OpenSourceS.md\n",
      "2023-09-04 07:36:34,903 - build_filename - INFO - data/blogs/2023-08-29-Enterprise_Low_Code-Plattformen_.md\n",
      "2023-09-04 07:36:34,904 - build_filename - INFO - data/blogs/2023-08-28-Fur_unsere_A_Low_Code-Plattform_.md\n",
      "2023-09-04 07:36:34,905 - build_filename - INFO - data/blogs/2023-08-28-In_einer_Software-getriebenen_Or.md\n",
      "2023-09-04 07:36:34,907 - build_filename - INFO - data/blogs/2023-08-28-Die_modellbasierte_Softwareentwi.md\n",
      "2023-09-04 07:36:34,908 - build_filename - INFO - data/blogs/2023-08-28-Die_Offenheit_der_mgm_A_Low-Code.md\n",
      "2023-09-04 07:36:34,909 - build_filename - INFO - data/blogs/2023-08-28-Bei_der_Sicherheitsplanung_in_ei.md\n",
      "2023-09-04 07:36:34,910 - build_filename - INFO - data/blogs/2023-08-21-Programmierung_ist_ein_wichtiger.md\n",
      "2023-09-04 07:36:34,912 - build_filename - INFO - data/blogs/2023-08-21-Die_Globalisierung_hat_in_den_le.md\n",
      "2023-09-04 07:36:34,913 - build_filename - INFO - data/blogs/2023-08-28-Um_die_nationalen_und_internatio.md\n",
      "2023-09-04 07:36:34,914 - build_filename - INFO - data/blogs/2023-08-05-Die_Bedeutung_einer_nahtlosen_Cu.md\n",
      "2023-09-04 07:36:34,915 - build_filename - INFO - data/blogs/2023-08-05-In_der_neuen_Ausgabe_von_DIE_MAC.md\n",
      "2023-09-04 07:36:34,916 - build_filename - INFO - data/blogs/2023-08-21-Heute_Grenoble_morgen_Hamburg_Da.md\n",
      "2023-09-04 07:36:34,917 - build_filename - INFO - data/blogs/2023-08-21-Der_Bedarf_an_schnellen_digitale.md\n",
      "2023-09-04 07:36:34,918 - build_filename - INFO - data/blogs/2023-07-06-This_weekend_the_Porto_team_got_.md\n",
      "2023-09-04 07:36:34,920 - build_filename - INFO - data/blogs/2023-07-06-Dicke_Bretter_nochmals_zum_Schlu.md\n",
      "2023-09-04 07:36:34,921 - build_filename - INFO - data/blogs/2023-08-21-Nachhaltige_Softwareprojekte_kon.md\n",
      "2023-09-04 07:36:34,923 - build_filename - INFO - data/blogs/2023-07-06-Wir_freuen_uns_auf_die_K_Future_.md\n",
      "2023-09-04 07:36:34,925 - build_filename - INFO - data/blogs/2023-08-21-Die_Fahigkeit_auf_Veranderungen_.md\n"
     ]
    }
   ],
   "source": [
    "def save_blog_post_to_file(blog_post: Dict) -> None:\n",
    "    content = blog_post[\"text\"]\n",
    "    filename = build_filename(blog_post)\n",
    "    frontmatter = build_frontmatter(blog_post)\n",
    "    path = os.path.dirname(filename)\n",
    "    #log(\"saveToFile\", \"Saving to file \", filename)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(frontmatter)\n",
    "        file.write(content)\n",
    "        file.close()\n",
    "\n",
    "def save_blog_posts_to_file(blog_posts):\n",
    "    for blog_post in blog_posts:\n",
    "        save_blog_post_to_file(blog_post)\n",
    "\n",
    "save_blog_posts_to_file(blog_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca29dc-b834-4597-83d0-8ef95dad6bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
